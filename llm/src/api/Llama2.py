# Importing necessary classes
from llm.src.conf.Configurations import logger
from langchain_core.prompts import PromptTemplate
from llm.src.utilities.Llama2Pipeline import Llama2Pipeline


def llama2_chat_ggu_question_answering(context: str, question: str):
    """
    Perform question answering using the Llama2ChatGGUF model
    :param context: The context in which to answer the question.
    :param question: The question to be answered based on the context.
    :return: The answer generated by the Llama2ChatGGUF model.
    """

    logger.info("Received a request to perform question answering using Llama2ChatGGUF model")

    # Enforcing that the model should strictly answer from context or say "I don't know"
    pre_prompt = """[INST] <<SYS>>"Answer based on context<<SYS>>"""

    template = pre_prompt + f"CONTEXT:\n{context}\n" + f"QUESTION:\n{question}\n" + "[INST]"

    print(template)

    prompt = PromptTemplate(template=template)

    print(prompt)
    llm = Llama2Pipeline().get_llm_model()

    # Generate the response from the model
    response = llm.invoke(prompt.format(context=context, question=question)).strip()

    return response


if __name__ == "__main__":
    # Sample context and question
    sample_context = "The capital of France is Paris. The Eiffel Tower is located in Paris."
    sample_question = "What is the capital of France??"

    # Perform question answering using the Llama2ChatGGUF model
    res = llama2_chat_ggu_question_answering(sample_context, sample_question)

    # Print the response
    print(res)
