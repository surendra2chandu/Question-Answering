# Importing necessary classes
from fastapi import HTTPException
from llm.src.conf.Configurations import logger
from llm.src.utilities.GGUFModelLoader import GGUFModelLoader
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_community.llms import HuggingFacePipeline


def llama2_chat_ggu_question_answering(context: str, question: str):
    """
    Perform question answering using the Llama2ChatGGUF model
    :param context: The context in which to answer the question.
    :param question: The question to be answered based on the context.
    :return: The answer generated by the Llama2ChatGGUF model.
    """

    logger.info("Received a request to perform question answering using Llama2ChatGGUF model")

    # Load the GGUF model using llama.cpp
    logger.info("Loading the Llama2ChatGGUF model...")
    llm = GGUFModelLoader().get_llm_model()

    # Prepare the input as a single string
    logger.info("Preparing the input for the model...")
    input_text = f"Context: {context}\nQuestion: {question}\nAnswer:"

    # Convert the input_text to bytes (UTF-8 encoding)
    logger.info("Converting the input text to bytes...")
    input_text_bytes = input_text.encode('utf-8')

    # Tokenize the input (pass bytes and integer for add_bos)
    try:
        logger.info("Tokenizing the input text...")
        tokens = llm.tokenize(input_text_bytes, 0)  # 0 for False (no beginning of sequence token)
    except Exception as e:
        logger.info(f"Error during tokenization: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error during tokenization: {str(e)}")

    # Generate response by passing tokens instead of raw string
    try:
        logger.info("Generating response using the model...")
        response = llm.create_completion(tokens, max_tokens=256)

    except Exception as e:
        logger.info(f"Error during model completion: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error during model completion: {str(e)}")

    return response


if __name__ == "__main__":
    # Sample context and question
    sample_context = "In the solar system, there are eight primary planets that orbit the sun."
    sample_question = "Name the eight planets in the solar system?"

    # Perform question answering using the Llama2ChatGGUF model
    res = llama2_chat_ggu_question_answering(sample_context, sample_question)

    # Print the response
    print(res)
