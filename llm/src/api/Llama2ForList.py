# Importing necessary classes
from llm.src.conf.Configurations import logger
from langchain_core.prompts import PromptTemplate
from llm.src.utilities.Llama2Pipeline import Llama2Pipeline

# Just an example for checking how llm responds for list of  questions

def llama2_chat_ggu_question_answering(context: str, questions: list):
    """
    Perform question answering using the Llama2ChatGGUF model for a list of questions with a single model call.
    :param context: The context in which to answer the questions.
    :param questions: A list of questions to be answered based on the context.
    :return: The raw response generated by the Llama2ChatGGUF model.
    """

    logger.info("Received a request to perform question answering using Llama2ChatGGUF model for multiple questions")

    # Enforcing that the model should strictly answer from context or say "I don't know"
    pre_prompt = """[INST] <<SYS>>\nYou are a concise assistant. Only use the given context to answer the following questions.
    If the answer is not in the context, reply with exactly "I don't know". 
    Your answer must strictly use words found only in the context. 
    Do not guess or generate answers from external knowledge. Your answer must be between 2 and 10 words.\n<<SYS>>\n\n"""

    # Combine all questions into a single prompt with clear separation
    combined_questions = "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])
    full_prompt = pre_prompt + f"CONTEXT:\n{context}\n\nQUESTIONS:\n{combined_questions}\nANSWERS:\n"

    prompt = PromptTemplate(template=full_prompt, input_variables=["context"])
    llm = Llama2Pipeline().get_llm_model()

    # Generate the response from the model
    response = llm.invoke(prompt).strip()

    return response


if __name__ == "__main__":
    # Sample context and list of questions
    sample_context = "In the solar system, there are eight primary planets that orbit the sun."
    sample_questions = [
        "Name the eight planets in the solar system?",
        "What is the largest planet in the solar system?",
        "How many gas giants are in the solar system?"
    ]

    # Perform question answering using the Llama2ChatGGUF model for multiple questions
    res = llama2_chat_ggu_question_answering(sample_context, sample_questions)

    # Print the raw response from the model
    print(res)
