# Import the necessary classes
from fastapi import HTTPException
from langchain_core.prompts import ChatPromptTemplate
from llm.src.conf.Prompts import default_prompt1
from llm.src.utilities.OllamaPipeline import OllamaPipeline
from llm.src.conf.Configurations import logger


def qa_with_ollama(context: str, question: str):
    """
    Perform question answering using the Ollama model
    :param context: The context in which to answer the question.
    :param question: The question to answer.
    :return: The answer generated by the Ollama model.
    """

    # Load the Ollama model
    model = OllamaPipeline().get_model()
    logger.info("Model initialized.")

    pre_prompt = f"""{default_prompt1}"""

    # Define the prompt template
    template = pre_prompt + """CONTEXT:{context}

    Question: {question}

    Answer: Just give a simple and precise answer. If the answer cannot be found in the context, respond with "Answer not found in context"."""
    prompt = ChatPromptTemplate.from_template(template)


    # Invoke the chain with a question
    try:
        # Format the prompt with the question input
        formatted_prompt = prompt.format_prompt(question=question, context=context)

        # Directly invoke the model with the formatted prompt
        logger.info("invoking the model with the formatted prompt")
        simple_response = model.invoke(formatted_prompt)
        logger.info("response received from the model")

        return simple_response

    except Exception as e:

        raise HTTPException(status_code=500, detail=f"An error occurred during invocation: {e}")


if __name__ == "__main__":

    sample_context = "The capital of France is Paris. The Eiffel Tower is located in Paris. India is a country in Asia."
    sample_question = "What is capital of France?"

    res = qa_with_ollama(sample_context, sample_question)

    print(res)




