# Importing necessary classes
from llm.src.utilities.OllamaPipeline import OllamaPipeline
from llm.src.conf.Configurations import logger
from fastapi import HTTPException

def qa_with_ollama(context: str, question: str):
    """
    Perform question answering using the Ollama model
    :param context: The context in which to answer the question.
    :param question: The question to answer.
    :return: The answer generated by the Ollama model.
    """

    # Load the Ollama model
    model = OllamaPipeline().get_model()
    logger.info("Model initialized.")

    # Define messages in the chat format with "system," "user," and "content"
    messages = [
        {"role": "system",
         "content": "You are a helpful assistant. Provide a concise and precise answer. If the answer cannot be found in the context, respond with 'Answer not found in context'."},
        {"role": "user", "content": "what is data science?"},
        {"role": "assistant",
         "content": "The capital of France is Paris. The Eiffel Tower is located in Paris. India is a country in Asia."}
    ]

    # Invoke the model with the chat structure
    try:

        # Directly invoke the model with the formatted prompt
        logger.info("invoking the model with the formatted prompt")
        response = model.invoke(input=messages)
        logger.info("response received from the model")

        return response

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred during invocation: {e}")


if __name__ == "__main__":

        sample_context = "The capital of France is Paris. The Eiffel Tower is located in Paris. India is a country in Asia."
        sample_question = "What is capital of France?"

        res = qa_with_ollama(sample_context, sample_question)

        print(res)


